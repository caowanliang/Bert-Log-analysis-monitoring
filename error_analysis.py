#!/usr/bin/env python
# -*- coding:utf-8 -*-
# @Time  : 2024/6/25 12:48
# @Author: william.cao
# @File  : error_analysis.py
# berté”™è¯¯æ—¥å¿—åˆ†æ
import os
import re
import torch
import logging

from model_training import initialize_model_and_tokenizer, load_config
from transformers import RobertaTokenizer, RobertaForSequenceClassification

# é…ç½®æ—¥å¿—è®°å½•å™¨
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œåªæ‰§è¡Œä¸€æ¬¡
model, tokenizer = None, None

def preprocess_log_message(log_message):
    """
    é¢„å¤„ç†æ—¥å¿—æ¶ˆæ¯ï¼Œå»é™¤æ— å…³ä¿¡æ¯å¹¶æå–æœ‰ç”¨ç‰¹å¾ã€‚
    :param log_message: åŸå§‹æ—¥å¿—æ¶ˆæ¯å­—ç¬¦ä¸²
    :return: é¢„å¤„ç†åçš„æ—¥å¿—æ¶ˆæ¯å­—ç¬¦ä¸²
    """
    if log_message is None:
        return log_message

    # å»é™¤æ—¶é—´æˆ³ï¼ˆåŒ…æ‹¬æ¯«ç§’éƒ¨åˆ†ï¼‰
    log_message = re.sub(r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}(?:\.\d{3})?', '', log_message)
    # å»é™¤ IP åœ°å€
    log_message = re.sub(r'\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b', '', log_message)
    # å»é™¤ä¸­æ‹¬å·å†…å®¹
    log_message = re.sub(r'\[.*?\]', '', log_message)
    # å»é™¤ ANSI è½¬ä¹‰åºåˆ—
    log_message = re.sub(r'\x1B[@-_][0-?]*[ -/]*[@-~]', '', log_message)
    # å»é™¤å¤šä½™çš„è½¬ä¹‰å­—ç¬¦ï¼ˆå¦‚ ï¼‰
    log_message = re.sub(r'\x1b', '', log_message)
    # å»é™¤æ‰€æœ‰æ•°å­—
    log_message = re.sub(r'\d+', '', log_message)

    return log_message.strip()


async def analyze_log_with_codebert(log_message):
    """ä½¿ç”¨CodeBERTæ¨¡å‹åˆ†ææ—¥å¿—æ¶ˆæ¯"""
    try:
        model_path = "E:\\Bert-Log-analysis-monitoring\\Bert-Log-analysis-monitoring\\codebert-base"
        
        # 1. å…ˆåŠ è½½æ¨¡å‹åˆ°CPU
        model = RobertaForSequenceClassification.from_pretrained(
            model_path,
            num_labels=2,
            torch_dtype=torch.float32  # æ˜ç¡®æŒ‡å®šæ•°æ®ç±»å‹
        )
        tokenizer = RobertaTokenizer.from_pretrained(model_path)
        
        # 2. æ£€æŸ¥è®¾å¤‡å¹¶æ­£ç¡®è½¬ç§»æ¨¡å‹
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        if device.type == 'cuda':
            # å¯¹äºGPUä½¿ç”¨to_empty()æ–¹æ³•
            model = model.to_empty(device=device)
            # é‡æ–°åŠ è½½æ¨¡å‹æƒé‡
            state_dict = torch.load(
                os.path.join(model_path, 'pytorch_model.bin'),
                map_location=device
            )
            model.load_state_dict(state_dict)
        else:
            # CPUç›´æ¥è½¬ç§»
            model = model.to(device)
            
        # 3. æ‰§è¡Œæ¨ç†
        inputs = tokenizer(log_message, return_tensors="pt", padding=True, truncation=True)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            confidence = torch.sigmoid(logits).item()
            
        return confidence
        
    except Exception as e:
        logging.error(f"CodeBERTåˆ†æå¤±è´¥: {e}")
        return None
